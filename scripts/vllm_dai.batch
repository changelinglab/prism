#!/bin/bash
#SBATCH -A bbjs-dtai-gh
#SBATCH -p ghx4
#SBATCH -J main
#SBATCH -N 1
#SBATCH --gpus-per-node=1
#SBATCH -c 72
#SBATCH --mem=120G
#SBATCH -t 2:00:00
#SBATCH -o exp/slurm_logs/%A_%a.out
#SBATCH -e exp/slurm_logs/%A_%a.out

##########################
echo "RUNNING ON NODE: $(hostname)"
echo "START TIME: $(date)"
echo "SLURM_JOB_ID: $SLURM_JOB_ID"
echo "RunningCMD: python src/main.py $*"
# === Directory Setup ===
PROJECT_HOME=/work/nvme/bbjs/sbharadwaj/powsm/PhoneBench
cd $PROJECT_HOME
mkdir -p "exp/slurm_logs"
# === Environment setup ===
source ~/.bashrc
conda deactivate
source setup_uv.sh .venv_dai requirements-dai.txt
# for w2v2ph these must be pre-built
export HF_HOME="${PROJECT_HOME}/exp/cache/hf"
export PHONEMIZER_ESPEAK_LIBRARY="/work/nvme/bbjs/sbharadwaj/powsm/dai_dependencies/espeak-ng/src/.libs/libespeak-ng.so.1.1.51"
export ESPEAK_DATA_PATH="/work/nvme/bbjs/sbharadwaj/powsm/dai_dependencies/espeak-ng/espeak-ng-data"
###########################
###########################
get_free_port_atomic() {
    local port
    local lock_base_dir="/dev/shm/vllm_port_locks"
    mkdir -p "$lock_base_dir"
    while true; do
        port=$(shuf -i 20000-60000 -n 1)
        if ! ss -lnt | grep -q ":$port "; then
            if mkdir "$lock_base_dir/$port" 2>/dev/null; then
                echo $port
                return 0
            fi
        fi
    done
}
###########################
###########################
nvidia-smi
# FREE_PORT=$(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1]); s.close()')
FREE_PORT=$(get_free_port_atomic)
echo "Using Port: $FREE_PORT"
VLLM_EXECUTABLE=exp/download/vllm_arm.sif
# MODEL=Qwen/Qwen3-Omni-30B-A3B-Thinking
# TOTAL_LENGTH=8192
# MODEL=nvidia/audio-flamingo-3-hf
MODEL=Qwen/Qwen3-Omni-30B-A3B-Instruct
TOTAL_LENGTH=4096
###########################
###########################
# Function to wait for vLLM service to be ready
wait_for_vllm_service() {
    local port=$1
    local service_pid=$2

    echo "Waiting for vLLM service to be ready on port ${port}..."
    local max_attempts=180  # 180 * 10 seconds = 30 minutes max wait
    local attempt=0

    while [ ${attempt} -lt ${max_attempts} ]; do
        # Check if service process is still running
        if ! kill -0 ${service_pid} 2>/dev/null; then
            echo "ERROR: Service process died unexpectedly"
            exit 1
        fi

        # Try to connect to the health endpoint
        if curl -s -o /dev/null -w "%{http_code}" \
            http://localhost:${port}/health 2>/dev/null | grep -q "200"; then
            echo "Service is ready!"

            # Double-check with models endpoint
            if curl -s http://localhost:${port}/v1/models 2>/dev/null | \
                grep -q "${MODEL#*/}"; then
                echo "Model loaded successfully"
                return 0
            fi
        fi

        attempt=$((attempt + 1))
        if [ ${attempt} -eq ${max_attempts} ]; then
            echo "ERROR: Service failed to start after 30 minutes"
            kill -9 ${service_pid} 2>/dev/null || true
            exit 1
        fi

        echo "Attempt ${attempt}/${max_attempts}: Service not ready yet, waiting 10s..."
        sleep 10
    done
}
###########################
###########################
apptainer exec --cleanenv --nv \
  --env HF_HOME="$HF_HOME" \
  -B $PROJECT_HOME \
  "${VLLM_EXECUTABLE}" \
  vllm serve ${MODEL} \
    --host 0.0.0.0 \
    --port $FREE_PORT \
    --trust-remote-code \
    --max-model-len ${TOTAL_LENGTH} \
    --gpu-memory-utilization 0.90 \
    --dtype bfloat16 \
    --enforce-eager > /dev/null 2>&1 &
SERVICE_PID=$!

trap "kill $SERVICE_PID" EXIT
echo "Service launched with PID ${SERVICE_PID}"
wait_for_vllm_service ${FREE_PORT} ${SERVICE_PID}
nvidia-smi
###########################
###########################
# A background function to log metrics every 10 seconds to a CSV
monitor_metrics() {
    local port=$1
    local out_file="exp/tmp/metrics_${port}.csv"
    mkdir -p "$(dirname "$out_file")"
    echo "Time,Running_Reqs,Waiting_Reqs,GPU_KV_Usage" > "$out_file"
    while true; do
        metrics=$(curl -s http://localhost:${port}/metrics)
        if [ -n "$metrics" ]; then
            running=$(echo "$metrics" | grep "^vllm:num_requests_running" | awk '{print $NF}')
            waiting=$(echo "$metrics" | grep "^vllm:num_requests_waiting" | awk '{print $NF}')
            gpu=$(echo "$metrics" | grep "^vllm:gpu_cache_usage_perc" | awk '{print $NF}')
            timestamp=$(date +%T)
            echo "$timestamp,$running,$waiting,$gpu" >> "$out_file"
        fi
        sleep 10
    done
}

monitor_metrics $FREE_PORT &
MONITOR_PID=$!
trap "kill $MONITOR_PID" EXIT
echo "Metrics will be available in exp/tmp/metrics_${FREE_PORT}.csv"
###########################
###########################
python src/main.py "$@" inference.port=${FREE_PORT}
###########################
###########################
# python src/main.py inference.port=43019 experiment=inference/transcribe_qweninstruct data=speechocean task_name=inf_speechocean_qweninstruct run_folder=4jobArr inference.limit_samples=4

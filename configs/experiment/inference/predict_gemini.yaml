# @package _global_

# Base config for Gemini direct-prompt downstream task inference.
# This config executes inference with Gemini for classification/regression/geolocation tasks.
#
# Set data and prompt fields when calling:
# eg. python src/main.py experiment=inference/predict_gemini data=cmul2arcticl1 prompt=l1cls_cmul2arctic task_name=dp_gemini_l1cls_cmul2arctic
#
# Reference: configs/experiment/inference/transcribe_gemini.yaml

defaults:
  - /prompt: ???  # REQUIRED: specify prompt config (e.g., l1cls_cmul2arctic)
  - override /logger: csv
  - override /model: null
  - override /model/net: null

task_name: ??? # pass dp_gemini_<task>_<data> when calling. This will create output dirs accordingly
tags: ["gemini", "inference", "zeroshot"]

seed: 42

train: false
test: false
distributed_predict: True

# Override data module to use test split only
data:
  predict_splits: ["test"]

inference:
  num_workers: 1  # NOTE: Keep num_workers=1 to ensure cache_path/error_log_path writes happen in a single process
  passthrough_keys: ["target", "split", "utt_id", "metadata_idx"]
  out_file: ${paths.output_dir}/prediction.jsonl
  limit_samples: null  # Set to null for full run

  inference_runner:
    _target_: src.model.gemini.predict.DirectPromptInference
    prompting:
      mode: zero_shot # zero_shot | few_shot
      seed: ${seed}
      # used when mode=few_shot (datamodule for support examples, usually same as ${data})
      support_data_cfg: ${data}
    client_config:
      model_name: "gemini-2.5-flash"
      api_key: ${oc.env:GEMINI_API_KEY}
      temperature: 1.0
      top_p: 1.0
      seed: ${seed}
      thinking_budget: 0  # Set to 0 for non-thinking mode
      retry_config:
        max_retries: 3
        initial_delay: 1.0
        backoff_factor: 2.0
    cache_key_field: "metadata_idx"
    cache_path: "${paths.output_dir}/prediction.cache.jsonl"
    error_log_path: "${paths.output_dir}/prediction.errors.jsonl"
    resume: True

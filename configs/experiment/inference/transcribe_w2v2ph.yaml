# @package _global_

# This config executes inference with wav2vec2-phoneme:
# Set data field when calling
# eg. python src/main.py experiment=inference/transcribe_w2v2ph data=kl_speechocean task_name=inf_kl_speechocean_ctag inference.inference_runner.hf_repo=ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns

defaults:
  - override /logger: csv
  - override /model: null
  - override /model/net: null

task_name: ??? # pass inf_<data>_<model> when calling. This will create output dirs accordingly
tags: ["w2v2ph","inference"]

seed: 42

train: false
test: false
distributed_predict: True

inference:
  num_workers: 15 # number of parallel workers for distributed inference, set acc to gpu memory
  passthrough_keys: ["target", "split", "utt_id", "metadata_idx", "lang_sym"]
  out_file: ${paths.output_dir}/transcription.json
  # an object (or fn that creates such an object) with __call__ method to transcribe single speech
  inference_runner:
    _target_: src.model.wav2vec2phoneme.builders.build_wav2vec2phoneme_inference
    hf_repo: ???
    # facebook/wav2vec2-lv-60-espeak-cv-ft
    # facebook/wav2vec2-xlsr-53-espeak-cv-ft
    # ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns
    device: cuda # auto, cpu, cuda